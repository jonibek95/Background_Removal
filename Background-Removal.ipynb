{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n",
      "YOLOv5m summary: 290 layers, 21172173 parameters, 0 gradients\n",
      "Detecting objects: 100%|██████████| 98/98 [00:00<00:00, 731.80it/s]\n",
      "Applying segmentation: 100%|██████████| 98/98 [00:34<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing video.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# REMOVAL BACKGROUND FOR VIDEO\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "sys.path.append(\"..\")  # Adjust path accordingly\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from yolov5.detect import YOLODetector\n",
    "\n",
    "# Initialize the detector and the SAM model\n",
    "detector = YOLODetector(weights='./yolov5/yolov5m.pt')\n",
    "sam_checkpoint = \"vit_h.pth\"\n",
    "model_type = \"vit_h\"\n",
    "device = \"cuda\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "# Define the video file to process\n",
    "video_path = './input/golf6.mp4'\n",
    "output_video_path = './output/masked_video.avi'\n",
    "\n",
    "# Open the input video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error opening video file\")\n",
    "    sys.exit()\n",
    "\n",
    "frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))))\n",
    "\n",
    "frames = []  \n",
    "bounding_boxes = []  \n",
    "\n",
    "# First phase: Detection\n",
    "for _ in tqdm(range(frame_count), desc=\"Detecting objects\"):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    frames.append(frame)  \n",
    "\n",
    "boxes = detector.Prediction(video_path)\n",
    "\n",
    "# Second phase: Segmentation\n",
    "for frame, boxes in tqdm(zip(frames, boxes), total=len(frames), desc=\"Applying segmentation\"):\n",
    "    if boxes:  \n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        predictor.set_image(rgb_frame)\n",
    "        masks, _, _ = predictor.predict(\n",
    "            point_coords=None, point_labels=None, box=np.array([boxes])[None, :], multimask_output=False)\n",
    "        final_mask = masks[0] > 0.3\n",
    "        final_mask = np.stack([final_mask*255]*3, axis=-1)\n",
    "        # segmented_frame = (frame * final_mask).astype(np.uint8)\n",
    "        segmented_frame = (final_mask).astype(np.uint8)\n",
    "        out.write(segmented_frame)\n",
    "    else:\n",
    "        out.write(frame)  \n",
    "\n",
    "# Cleanup\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"Finished processing video.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import psutil\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# from track_anything import TrackingAnything, parse_augment\n",
    "from metaseg import SegAutoMaskPredictor, SegManualMaskPredictor, SahiAutoSegmentation, sahi_sliced_predict\n",
    "from segment_anythingss import sam_model_registry, SamPredictor\n",
    "from yolov5m.detect import YOLODetector\n",
    "\n",
    "\n",
    "sam_checkpoint = \"vit_h.pth\"\n",
    "model_type = \"vit_h\"\n",
    "device = \"cuda\"\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "def get_frames_from_video(video_path):\n",
    "\n",
    "    frames = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            current_memory_usage = psutil.virtual_memory().percent\n",
    "            frames.append(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "            if current_memory_usage > 90:\n",
    "                operation_log = [(\"Memory usage is too high (>90%). Stop the video extraction. Please reduce the video resolution or frame rate.\", \"Error\")]\n",
    "                print(\"Memory usage is too high (>90%). Please reduce the video resolution or frame rate.\")\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    cap.release()\n",
    "    return frames\n",
    "\n",
    "def process_frame_with_sahi(frame_path, sam_model_type, detection_model_path, output_mask_dir):   \n",
    "    os.makedirs(output_mask_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "    boxes = sahi_sliced_predict(\n",
    "        image_path=frame_path,\n",
    "        detection_model_type=\"yolov8\",\n",
    "        detection_model_path=detection_model_path,\n",
    "        conf_th=0.5,\n",
    "        image_size=640,\n",
    "        slice_height=256,\n",
    "        slice_width=256,\n",
    "        overlap_height_ratio=0.2,\n",
    "        overlap_width_ratio=0.2\n",
    "    )\n",
    "\n",
    "    sahi_segmentation = SahiAutoSegmentation()\n",
    "    mask_image = sahi_segmentation.predict(\n",
    "        source=frame_path,\n",
    "        model_type=sam_model_type,\n",
    "        input_box=boxes,\n",
    "        multimask_output=False,\n",
    "        random_color=False,\n",
    "        show=False,\n",
    "        save=False\n",
    "    )\n",
    "    # cv2.imwrite(\"mask_image.png\", mask_image)\n",
    "    frame = cv2.imread(frame_path)\n",
    "    \n",
    "    if isinstance(mask_image, torch.Tensor):\n",
    "        mask_image_np = mask_image.cpu().numpy()\n",
    "    else:\n",
    "        mask_image_np = np.array(mask_image)\n",
    "\n",
    "    # Ensure mask is correctly formed\n",
    "    if len(mask_image_np.shape) == 4:\n",
    "        # Flatten the first two dimensions if necessary\n",
    "        mask_np = mask_image_np.reshape(-1, mask_image_np.shape[2], mask_image_np.shape[3])\n",
    "\n",
    "        mask_np = mask_np[0]  # Taking the first mask as an example\n",
    "    elif len(mask_image_np.shape) == 3:\n",
    "        mask_np = mask_image_np[0]\n",
    "    else:\n",
    "        print(f\"Unexpected mask shape: {mask_image_np.shape}\")\n",
    "        mask_np = np.zeros_like(frame, dtype=np.uint8)\n",
    "\n",
    "    if mask_np.dtype != np.uint8:\n",
    "        mask_np = mask_np.astype(np.uint8) * 255\n",
    "\n",
    "    return mask_np\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    frames = get_frames_from_video(\"./input/golf6.mp4\")\n",
    "    \n",
    "    temp_frames_dir = './input/frames'\n",
    "    temp_masks_dir = './input/masks'\n",
    "\n",
    "    # Clearing and creating the 'frames' directory\n",
    "    if os.path.exists(temp_frames_dir):\n",
    "        shutil.rmtree(temp_frames_dir)\n",
    "    os.makedirs(temp_frames_dir, exist_ok=True)\n",
    "\n",
    "    # Clearing and creating the 'masks' directory\n",
    "    if os.path.exists(temp_masks_dir):\n",
    "        shutil.rmtree(temp_masks_dir)\n",
    "    os.makedirs(temp_masks_dir, exist_ok=True)\n",
    "\n",
    "    for i in tqdm(range(len(frames)), desc=\"Processing frames and masks\"):\n",
    "        frame = frames[i]\n",
    "        frame_path = os.path.join(temp_frames_dir, f'{i:05d}.png')\n",
    "        bgr_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert back to BGR for saving\n",
    "        cv2.imwrite(frame_path, bgr_frame)\n",
    "\n",
    "        mask_np = process_frame_with_sahi(\n",
    "            frame_path=frame_path,\n",
    "            sam_model_type=\"vit_h\",\n",
    "            detection_model_path=\"./checkpoints/yolov8n.pt\",\n",
    "            output_mask_dir=temp_masks_dir,\n",
    "        )\n",
    "        # print('dddd: ', mask_np)\n",
    "        # mask_np = cv2.resize(mask_np, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "        # mask_filename = f\"{i:05d}.png\"\n",
    "        # cv2.imwrite(os.path.join(temp_masks_dir, mask_filename), mask_np)\n",
    "        # if mask_np.ndim == 2:\n",
    "        #     mask_np = np.stack([mask_np]*3, axis=-1)\n",
    "        # elif mask_np.ndim == 3 and mask_np.shape[2] == 1:\n",
    "        #     mask_np = np.concatenate([mask_np]*3, axis=-1)\n",
    "        \n",
    "        # mask_np = (mask_np * 255).astype(np.uint8)\n",
    "        # print(mask_np.shape)\n",
    "        # mask_np = cv2.resize(mask_np, (frame.shape[1], frame.shape[0]))\n",
    "\n",
    "        # mask_filename = f\"{i:05d}.png\"\n",
    "        # cv2.imwrite(os.path.join(temp_masks_dir, mask_filename), mask_np)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
